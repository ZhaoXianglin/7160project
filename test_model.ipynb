{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are 13 features taking integer values (mostly count features) and 26 categorical features. The values of the categorical features have been hashed onto 32 bits for anonymization purposes."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lable', 'I1', 'I2', 'I3', 'I4', 'I5', 'I6', 'I7', 'I8', 'I9', 'I10', 'I11', 'I12', 'I13', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'C15', 'C16', 'C17', 'C18', 'C19', 'C20', 'C21', 'C22', 'C23', 'C24', 'C25', 'C26']\n"
     ]
    }
   ],
   "source": [
    "integer_features = ['I' + str(i) for i in range(1, 14)]\n",
    "categorical_features = [\"C\" + str(i) for i in range(1, 27)]\n",
    "data_titles = ['lable'] + integer_features + categorical_features\n",
    "print(data_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "   label   I1  I2    I3   I4       I5    I6    I7    I8     I9  ...       C17  \\\n0      1  1.0  25  32.0  8.0      1.0  10.0  22.0   7.0   23.0  ...  e5ba7672   \n1      0  NaN  -1   4.0  4.0   3348.0  12.0  82.0   7.0  164.0  ...  e5ba7672   \n2      0  NaN   1  13.0  3.0   4356.0  17.0   2.0   3.0   10.0  ...  07c540c4   \n3      0  NaN  28   1.0  NaN  11064.0  51.0   3.0   6.0   45.0  ...  07c540c4   \n4      0  3.0  -1   2.0  2.0    931.0  25.0   3.0  15.0   18.0  ...  776ce399   \n\n        C18       C19       C20       C21  C22       C23       C24       C25  \\\n0  6fc84bfb       NaN       NaN  5155d8a3  NaN  423fab69  ded4aac9       NaN   \n1  e7e991cb  21ddcdc9  a458ea53  1826f894  NaN  423fab69  3fdb382b  ea9a246c   \n2  7ef5affa  9908ba56  a458ea53  88d27c62  NaN  32c7478e  3fdb382b  001f3601   \n3  63aa00dd       NaN       NaN  424af181  NaN  3a171ecb  869caea3       NaN   \n4  0f2f9850  26e97973  a458ea53  d7c93a6d  NaN  32c7478e  a9a2ac1a  9b3e8820   \n\n        C26  \n0       NaN  \n1  be8b4aee  \n2  49d68486  \n3       NaN  \n4  3547c540  \n\n[5 rows x 40 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>I1</th>\n      <th>I2</th>\n      <th>I3</th>\n      <th>I4</th>\n      <th>I5</th>\n      <th>I6</th>\n      <th>I7</th>\n      <th>I8</th>\n      <th>I9</th>\n      <th>...</th>\n      <th>C17</th>\n      <th>C18</th>\n      <th>C19</th>\n      <th>C20</th>\n      <th>C21</th>\n      <th>C22</th>\n      <th>C23</th>\n      <th>C24</th>\n      <th>C25</th>\n      <th>C26</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1.0</td>\n      <td>25</td>\n      <td>32.0</td>\n      <td>8.0</td>\n      <td>1.0</td>\n      <td>10.0</td>\n      <td>22.0</td>\n      <td>7.0</td>\n      <td>23.0</td>\n      <td>...</td>\n      <td>e5ba7672</td>\n      <td>6fc84bfb</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>5155d8a3</td>\n      <td>NaN</td>\n      <td>423fab69</td>\n      <td>ded4aac9</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>NaN</td>\n      <td>-1</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>3348.0</td>\n      <td>12.0</td>\n      <td>82.0</td>\n      <td>7.0</td>\n      <td>164.0</td>\n      <td>...</td>\n      <td>e5ba7672</td>\n      <td>e7e991cb</td>\n      <td>21ddcdc9</td>\n      <td>a458ea53</td>\n      <td>1826f894</td>\n      <td>NaN</td>\n      <td>423fab69</td>\n      <td>3fdb382b</td>\n      <td>ea9a246c</td>\n      <td>be8b4aee</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>13.0</td>\n      <td>3.0</td>\n      <td>4356.0</td>\n      <td>17.0</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>10.0</td>\n      <td>...</td>\n      <td>07c540c4</td>\n      <td>7ef5affa</td>\n      <td>9908ba56</td>\n      <td>a458ea53</td>\n      <td>88d27c62</td>\n      <td>NaN</td>\n      <td>32c7478e</td>\n      <td>3fdb382b</td>\n      <td>001f3601</td>\n      <td>49d68486</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>NaN</td>\n      <td>28</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>11064.0</td>\n      <td>51.0</td>\n      <td>3.0</td>\n      <td>6.0</td>\n      <td>45.0</td>\n      <td>...</td>\n      <td>07c540c4</td>\n      <td>63aa00dd</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>424af181</td>\n      <td>NaN</td>\n      <td>3a171ecb</td>\n      <td>869caea3</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>3.0</td>\n      <td>-1</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>931.0</td>\n      <td>25.0</td>\n      <td>3.0</td>\n      <td>15.0</td>\n      <td>18.0</td>\n      <td>...</td>\n      <td>776ce399</td>\n      <td>0f2f9850</td>\n      <td>26e97973</td>\n      <td>a458ea53</td>\n      <td>d7c93a6d</td>\n      <td>NaN</td>\n      <td>32c7478e</td>\n      <td>a9a2ac1a</td>\n      <td>9b3e8820</td>\n      <td>3547c540</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 40 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data original data is tool large. used a 50w items data\n",
    "# train_data = pd.read_csv(\"data/train.txt\", names=data_titles, sep=\"\\t\")\n",
    "# train_data.head()\n",
    "original_data = pd.read_csv(\"data/criteo_sample_50w.csv\")\n",
    "original_data.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "   label     I1  I2    I3     I4       I5    I6    I7    I8     I9  ...  \\\n0      1    1.0  25  32.0    8.0      1.0  10.0  22.0   7.0   23.0  ...   \n1      0  -2022  -1   4.0    4.0   3348.0  12.0  82.0   7.0  164.0  ...   \n2      0  -2022   1  13.0    3.0   4356.0  17.0   2.0   3.0   10.0  ...   \n3      0  -2022  28   1.0  -2022  11064.0  51.0   3.0   6.0   45.0  ...   \n4      0    3.0  -1   2.0    2.0    931.0  25.0   3.0  15.0   18.0  ...   \n\n        C17       C18       C19       C20       C21 C22       C23       C24  \\\n0  e5ba7672  6fc84bfb         0         0  5155d8a3   0  423fab69  ded4aac9   \n1  e5ba7672  e7e991cb  21ddcdc9  a458ea53  1826f894   0  423fab69  3fdb382b   \n2  07c540c4  7ef5affa  9908ba56  a458ea53  88d27c62   0  32c7478e  3fdb382b   \n3  07c540c4  63aa00dd         0         0  424af181   0  3a171ecb  869caea3   \n4  776ce399  0f2f9850  26e97973  a458ea53  d7c93a6d   0  32c7478e  a9a2ac1a   \n\n        C25       C26  \n0         0         0  \n1  ea9a246c  be8b4aee  \n2  001f3601  49d68486  \n3         0         0  \n4  9b3e8820  3547c540  \n\n[5 rows x 40 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>I1</th>\n      <th>I2</th>\n      <th>I3</th>\n      <th>I4</th>\n      <th>I5</th>\n      <th>I6</th>\n      <th>I7</th>\n      <th>I8</th>\n      <th>I9</th>\n      <th>...</th>\n      <th>C17</th>\n      <th>C18</th>\n      <th>C19</th>\n      <th>C20</th>\n      <th>C21</th>\n      <th>C22</th>\n      <th>C23</th>\n      <th>C24</th>\n      <th>C25</th>\n      <th>C26</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1.0</td>\n      <td>25</td>\n      <td>32.0</td>\n      <td>8.0</td>\n      <td>1.0</td>\n      <td>10.0</td>\n      <td>22.0</td>\n      <td>7.0</td>\n      <td>23.0</td>\n      <td>...</td>\n      <td>e5ba7672</td>\n      <td>6fc84bfb</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5155d8a3</td>\n      <td>0</td>\n      <td>423fab69</td>\n      <td>ded4aac9</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>-2022</td>\n      <td>-1</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>3348.0</td>\n      <td>12.0</td>\n      <td>82.0</td>\n      <td>7.0</td>\n      <td>164.0</td>\n      <td>...</td>\n      <td>e5ba7672</td>\n      <td>e7e991cb</td>\n      <td>21ddcdc9</td>\n      <td>a458ea53</td>\n      <td>1826f894</td>\n      <td>0</td>\n      <td>423fab69</td>\n      <td>3fdb382b</td>\n      <td>ea9a246c</td>\n      <td>be8b4aee</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>-2022</td>\n      <td>1</td>\n      <td>13.0</td>\n      <td>3.0</td>\n      <td>4356.0</td>\n      <td>17.0</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>10.0</td>\n      <td>...</td>\n      <td>07c540c4</td>\n      <td>7ef5affa</td>\n      <td>9908ba56</td>\n      <td>a458ea53</td>\n      <td>88d27c62</td>\n      <td>0</td>\n      <td>32c7478e</td>\n      <td>3fdb382b</td>\n      <td>001f3601</td>\n      <td>49d68486</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>-2022</td>\n      <td>28</td>\n      <td>1.0</td>\n      <td>-2022</td>\n      <td>11064.0</td>\n      <td>51.0</td>\n      <td>3.0</td>\n      <td>6.0</td>\n      <td>45.0</td>\n      <td>...</td>\n      <td>07c540c4</td>\n      <td>63aa00dd</td>\n      <td>0</td>\n      <td>0</td>\n      <td>424af181</td>\n      <td>0</td>\n      <td>3a171ecb</td>\n      <td>869caea3</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>3.0</td>\n      <td>-1</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>931.0</td>\n      <td>25.0</td>\n      <td>3.0</td>\n      <td>15.0</td>\n      <td>18.0</td>\n      <td>...</td>\n      <td>776ce399</td>\n      <td>0f2f9850</td>\n      <td>26e97973</td>\n      <td>a458ea53</td>\n      <td>d7c93a6d</td>\n      <td>0</td>\n      <td>32c7478e</td>\n      <td>a9a2ac1a</td>\n      <td>9b3e8820</td>\n      <td>3547c540</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 40 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove null\n",
    "original_data[integer_features] = original_data[integer_features].fillna('-2022', )\n",
    "original_data[categorical_features] = original_data[categorical_features].fillna(0, )\n",
    "targets = ['label']\n",
    "original_data.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "    C1   C2      C3     C4   C5  C6    C7   C8  C9    C10  ...  C17   C18  \\\n0   20   10  103950  48164   37   6  3796   18   2  21865  ...    9  1541   \n1   20  249  108293  58867   37  13  4961  165   2  15314  ...    9  3188   \n2  383  168  109278  43314   37  13  3938   18   2   9543  ...    0  1727   \n3  760  312   84271  68069  160   6  9868   18   2  12169  ...    0  1371   \n4  760  451   17999  16618   37   6  4309  410   2   5368  ...    5   217   \n\n    C19  C20     C21  C22  C23    C24  C25    C26  \n0     0    0   43790    0    4  25283    0      0  \n1   212    2   13014    0    4   7205   54  16624  \n2  1037    2   73724    0    2   7205    1   6343  \n3     0    0   35768    0    3  15271    0      0  \n4   242    2  116420    0    2  19260   36   4586  \n\n[5 rows x 26 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>C1</th>\n      <th>C2</th>\n      <th>C3</th>\n      <th>C4</th>\n      <th>C5</th>\n      <th>C6</th>\n      <th>C7</th>\n      <th>C8</th>\n      <th>C9</th>\n      <th>C10</th>\n      <th>...</th>\n      <th>C17</th>\n      <th>C18</th>\n      <th>C19</th>\n      <th>C20</th>\n      <th>C21</th>\n      <th>C22</th>\n      <th>C23</th>\n      <th>C24</th>\n      <th>C25</th>\n      <th>C26</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>20</td>\n      <td>10</td>\n      <td>103950</td>\n      <td>48164</td>\n      <td>37</td>\n      <td>6</td>\n      <td>3796</td>\n      <td>18</td>\n      <td>2</td>\n      <td>21865</td>\n      <td>...</td>\n      <td>9</td>\n      <td>1541</td>\n      <td>0</td>\n      <td>0</td>\n      <td>43790</td>\n      <td>0</td>\n      <td>4</td>\n      <td>25283</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20</td>\n      <td>249</td>\n      <td>108293</td>\n      <td>58867</td>\n      <td>37</td>\n      <td>13</td>\n      <td>4961</td>\n      <td>165</td>\n      <td>2</td>\n      <td>15314</td>\n      <td>...</td>\n      <td>9</td>\n      <td>3188</td>\n      <td>212</td>\n      <td>2</td>\n      <td>13014</td>\n      <td>0</td>\n      <td>4</td>\n      <td>7205</td>\n      <td>54</td>\n      <td>16624</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>383</td>\n      <td>168</td>\n      <td>109278</td>\n      <td>43314</td>\n      <td>37</td>\n      <td>13</td>\n      <td>3938</td>\n      <td>18</td>\n      <td>2</td>\n      <td>9543</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1727</td>\n      <td>1037</td>\n      <td>2</td>\n      <td>73724</td>\n      <td>0</td>\n      <td>2</td>\n      <td>7205</td>\n      <td>1</td>\n      <td>6343</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>760</td>\n      <td>312</td>\n      <td>84271</td>\n      <td>68069</td>\n      <td>160</td>\n      <td>6</td>\n      <td>9868</td>\n      <td>18</td>\n      <td>2</td>\n      <td>12169</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1371</td>\n      <td>0</td>\n      <td>0</td>\n      <td>35768</td>\n      <td>0</td>\n      <td>3</td>\n      <td>15271</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>760</td>\n      <td>451</td>\n      <td>17999</td>\n      <td>16618</td>\n      <td>37</td>\n      <td>6</td>\n      <td>4309</td>\n      <td>410</td>\n      <td>2</td>\n      <td>5368</td>\n      <td>...</td>\n      <td>5</td>\n      <td>217</td>\n      <td>242</td>\n      <td>2</td>\n      <td>116420</td>\n      <td>0</td>\n      <td>2</td>\n      <td>19260</td>\n      <td>36</td>\n      <td>4586</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 26 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "original_data[categorical_features] = original_data[categorical_features].astype('str')\n",
    "for feature in categorical_features:\n",
    "    lbe = LabelEncoder()\n",
    "    # trans each feature to an id\n",
    "    original_data[feature] = lbe.fit_transform(original_data[feature])\n",
    "original_data[categorical_features].head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "         I1        I2        I3        I4        I5        I6        I7  \\\n0  0.852800 -0.192994  0.517242  0.589790 -0.271033  0.368287  0.222795   \n1 -1.169387 -0.269791  0.487845  0.585280 -0.220090  0.370272  0.368872   \n2 -1.169387 -0.263883  0.497294  0.584153 -0.204748  0.375234  0.174102   \n3 -1.169387 -0.184132  0.484695 -1.698848 -0.102649  0.408977  0.176537   \n4  0.854800 -0.269791  0.485745  0.583026 -0.256878  0.383173  0.176537   \n\n         I8        I9       I10       I11       I12       I13  \n0 -0.073269  0.004389  0.855558  0.221647 -0.549225  0.588038  \n1 -0.073269  0.298103 -1.169416  0.236494  1.819670  0.583535  \n2 -0.132192 -0.022691 -1.169416  0.204325 -0.549225  0.582409  \n3 -0.088000  0.050217 -1.169416  0.204325 -0.549225 -1.697306  \n4  0.044577 -0.006026  0.855558  0.204325 -0.549225  0.581284  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>I1</th>\n      <th>I2</th>\n      <th>I3</th>\n      <th>I4</th>\n      <th>I5</th>\n      <th>I6</th>\n      <th>I7</th>\n      <th>I8</th>\n      <th>I9</th>\n      <th>I10</th>\n      <th>I11</th>\n      <th>I12</th>\n      <th>I13</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.852800</td>\n      <td>-0.192994</td>\n      <td>0.517242</td>\n      <td>0.589790</td>\n      <td>-0.271033</td>\n      <td>0.368287</td>\n      <td>0.222795</td>\n      <td>-0.073269</td>\n      <td>0.004389</td>\n      <td>0.855558</td>\n      <td>0.221647</td>\n      <td>-0.549225</td>\n      <td>0.588038</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-1.169387</td>\n      <td>-0.269791</td>\n      <td>0.487845</td>\n      <td>0.585280</td>\n      <td>-0.220090</td>\n      <td>0.370272</td>\n      <td>0.368872</td>\n      <td>-0.073269</td>\n      <td>0.298103</td>\n      <td>-1.169416</td>\n      <td>0.236494</td>\n      <td>1.819670</td>\n      <td>0.583535</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-1.169387</td>\n      <td>-0.263883</td>\n      <td>0.497294</td>\n      <td>0.584153</td>\n      <td>-0.204748</td>\n      <td>0.375234</td>\n      <td>0.174102</td>\n      <td>-0.132192</td>\n      <td>-0.022691</td>\n      <td>-1.169416</td>\n      <td>0.204325</td>\n      <td>-0.549225</td>\n      <td>0.582409</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-1.169387</td>\n      <td>-0.184132</td>\n      <td>0.484695</td>\n      <td>-1.698848</td>\n      <td>-0.102649</td>\n      <td>0.408977</td>\n      <td>0.176537</td>\n      <td>-0.088000</td>\n      <td>0.050217</td>\n      <td>-1.169416</td>\n      <td>0.204325</td>\n      <td>-0.549225</td>\n      <td>-1.697306</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.854800</td>\n      <td>-0.269791</td>\n      <td>0.485745</td>\n      <td>0.583026</td>\n      <td>-0.256878</td>\n      <td>0.383173</td>\n      <td>0.176537</td>\n      <td>0.044577</td>\n      <td>-0.006026</td>\n      <td>0.855558</td>\n      <td>0.204325</td>\n      <td>-0.549225</td>\n      <td>0.581284</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalization\n",
    "original_data[integer_features] = original_data[integer_features].astype('float')\n",
    "for feature in integer_features:\n",
    "    mean = original_data[feature].mean()\n",
    "    std = original_data[feature].std()\n",
    "    original_data[feature] = (original_data[feature] - mean) / (std + 1e-12)  #remove 0\n",
    "\n",
    "original_data[integer_features].head()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "(500000, 40)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_data.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400000, 40) (100000, 40)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(original_data, test_size=0.2, random_state=2248)\n",
    "print(train.shape, test.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import torch.utils.data as Data\n",
    "\n",
    "train_data = Data.TensorDataset(torch.LongTensor(train[categorical_features].values),\n",
    "                                torch.FloatTensor(train[integer_features].values),\n",
    "                                torch.FloatTensor(train['label'].values), )\n",
    "\n",
    "train_loader = Data.DataLoader(dataset=train_data, batch_size=2048, shuffle=True)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "test_data = Data.TensorDataset(torch.LongTensor(test[categorical_features].values),\n",
    "                               torch.FloatTensor(test[integer_features].values),\n",
    "                               torch.FloatTensor(test['label'].values), )\n",
    "test_loader = Data.DataLoader(dataset=test_data, batch_size=2048, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cuda')"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "[1036,\n 526,\n 177397,\n 75762,\n 234,\n 14,\n 10165,\n 482,\n 3,\n 23499,\n 4501,\n 152106,\n 3029,\n 26,\n 7825,\n 120291,\n 10,\n 3512,\n 1704,\n 4,\n 138137,\n 14,\n 15,\n 29183,\n 63,\n 22354]"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cate_each_features_size = [original_data[f].nunique() for f in categorical_features]\n",
    "cate_each_features_size"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# init model\n",
    "from DeepFM import DeepFM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "DeepFM(\n  (fm_1st_int_vec): Linear(in_features=13, out_features=1, bias=True)\n  (fm_1st_cate_emb): ModuleList(\n    (0): Embedding(1036, 1)\n    (1): Embedding(526, 1)\n    (2): Embedding(177397, 1)\n    (3): Embedding(75762, 1)\n    (4): Embedding(234, 1)\n    (5): Embedding(14, 1)\n    (6): Embedding(10165, 1)\n    (7): Embedding(482, 1)\n    (8): Embedding(3, 1)\n    (9): Embedding(23499, 1)\n    (10): Embedding(4501, 1)\n    (11): Embedding(152106, 1)\n    (12): Embedding(3029, 1)\n    (13): Embedding(26, 1)\n    (14): Embedding(7825, 1)\n    (15): Embedding(120291, 1)\n    (16): Embedding(10, 1)\n    (17): Embedding(3512, 1)\n    (18): Embedding(1704, 1)\n    (19): Embedding(4, 1)\n    (20): Embedding(138137, 1)\n    (21): Embedding(14, 1)\n    (22): Embedding(15, 1)\n    (23): Embedding(29183, 1)\n    (24): Embedding(63, 1)\n    (25): Embedding(22354, 1)\n  )\n  (fm_2nd_cate_emb): ModuleList(\n    (0): Embedding(1036, 8)\n    (1): Embedding(526, 8)\n    (2): Embedding(177397, 8)\n    (3): Embedding(75762, 8)\n    (4): Embedding(234, 8)\n    (5): Embedding(14, 8)\n    (6): Embedding(10165, 8)\n    (7): Embedding(482, 8)\n    (8): Embedding(3, 8)\n    (9): Embedding(23499, 8)\n    (10): Embedding(4501, 8)\n    (11): Embedding(152106, 8)\n    (12): Embedding(3029, 8)\n    (13): Embedding(26, 8)\n    (14): Embedding(7825, 8)\n    (15): Embedding(120291, 8)\n    (16): Embedding(10, 8)\n    (17): Embedding(3512, 8)\n    (18): Embedding(1704, 8)\n    (19): Embedding(4, 8)\n    (20): Embedding(138137, 8)\n    (21): Embedding(14, 8)\n    (22): Embedding(15, 8)\n    (23): Embedding(29183, 8)\n    (24): Embedding(63, 8)\n    (25): Embedding(22354, 8)\n  )\n  (dense_linear): Linear(in_features=13, out_features=208, bias=True)\n  (linear_1): Linear(in_features=208, out_features=256, bias=True)\n  (Norm_1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (act_1): ReLU()\n  (dropout_1): Dropout(p=0.2, inplace=False)\n  (linear_2): Linear(in_features=256, out_features=128, bias=True)\n  (Norm_2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (act_2): ReLU()\n  (dropout_2): Dropout(p=0.2, inplace=False)\n  (relu): ReLU()\n  (sigmoid): Sigmoid()\n  (out_linear): Linear(in_features=128, out_features=1, bias=True)\n)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DeepFM(39, integer_features, categorical_features, cate_each_features_size,device='cuda')\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters 7037251\n"
     ]
    }
   ],
   "source": [
    "print(\"parameters\", sum(pa.numel() for pa in model.parameters()))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1669556079.104278\n",
      "Epoch:1, Step:50/196, Loss:20.14670824050903\n",
      "Epoch:1, Step:100/196, Loss:17.401493225097656\n",
      "Epoch:1, Step:150/196, Loss:14.837036097844441\n",
      "Epoch:1, Step:196/196, Loss:12.721613014230924\n",
      "Best AUC:0.6515834713560766\n",
      "1669556085.5180995\n",
      "Epoch:2, Step:50/196, Loss:2.5678839707374572\n",
      "Epoch:2, Step:100/196, Loss:1.9238552033901215\n",
      "Epoch:2, Step:150/196, Loss:1.5866546853383383\n",
      "Epoch:2, Step:196/196, Loss:1.4006596162002913\n",
      "Best AUC:0.6647373803188643\n",
      "1669556090.9835365\n",
      "Epoch:3, Step:50/196, Loss:0.6435158598423004\n",
      "Epoch:3, Step:100/196, Loss:0.6213021522760391\n",
      "Epoch:3, Step:150/196, Loss:0.6025592776139578\n",
      "Epoch:3, Step:196/196, Loss:0.5908416252659292\n",
      "Best AUC:0.7073969730364735\n",
      "1669556096.4097717\n",
      "Epoch:4, Step:50/196, Loss:0.5329968571662903\n",
      "Epoch:4, Step:100/196, Loss:0.5319780048727989\n",
      "Epoch:4, Step:150/196, Loss:0.5271916230519612\n",
      "Epoch:4, Step:196/196, Loss:0.5245649597170402\n",
      "Best AUC:0.7324031262428\n",
      "1669556101.6785994\n",
      "Epoch:5, Step:50/196, Loss:0.5011488801240921\n",
      "Epoch:5, Step:100/196, Loss:0.5013146278262138\n",
      "Epoch:5, Step:150/196, Loss:0.5004800875981649\n",
      "Epoch:5, Step:196/196, Loss:0.4996194369634803\n",
      "Best AUC:0.7457564004263418\n",
      "1669556107.1684055\n",
      "Epoch:6, Step:50/196, Loss:0.490810432434082\n",
      "Epoch:6, Step:100/196, Loss:0.4928442698717117\n",
      "Epoch:6, Step:150/196, Loss:0.4921920470396678\n",
      "Epoch:6, Step:196/196, Loss:0.4916595176470523\n",
      "Best AUC:0.7532177381655821\n",
      "1669556112.3720803\n",
      "Epoch:7, Step:50/196, Loss:0.4832800060510635\n",
      "Epoch:7, Step:100/196, Loss:0.4854754900932312\n",
      "Epoch:7, Step:150/196, Loss:0.4861673661073049\n",
      "Epoch:7, Step:196/196, Loss:0.4893464646777328\n",
      "Best AUC:0.7536995314231674\n",
      "1669556117.5297885\n",
      "Epoch:8, Step:50/196, Loss:0.4855410182476044\n",
      "Epoch:8, Step:100/196, Loss:0.4862209588289261\n",
      "Epoch:8, Step:150/196, Loss:0.4859961555401484\n",
      "Epoch:8, Step:196/196, Loss:0.48604280653656745\n",
      "Best AUC:0.7543642445735667\n",
      "1669556122.667778\n",
      "Epoch:9, Step:50/196, Loss:0.4804273808002472\n",
      "Epoch:9, Step:100/196, Loss:0.4838161009550095\n",
      "Epoch:9, Step:150/196, Loss:0.48562270204226177\n",
      "Epoch:9, Step:196/196, Loss:0.48551550309877006\n",
      "Best AUC:0.7569449196801159\n",
      "1669556127.8489287\n",
      "Epoch:10, Step:50/196, Loss:0.48166206657886507\n",
      "Epoch:10, Step:100/196, Loss:0.4831812369823456\n",
      "Epoch:10, Step:150/196, Loss:0.484389431476593\n",
      "Epoch:10, Step:196/196, Loss:0.485703103092252\n",
      "Best AUC:0.7569449196801159\n",
      "1669556133.0627527\n",
      "Epoch:11, Step:50/196, Loss:0.48214715123176577\n",
      "Epoch:11, Step:100/196, Loss:0.48367165148258207\n",
      "Epoch:11, Step:150/196, Loss:0.48515727937221526\n",
      "Epoch:11, Step:196/196, Loss:0.485407080729397\n",
      "Best AUC:0.7570560987953304\n",
      "1669556138.1607027\n",
      "Epoch:12, Step:50/196, Loss:0.4819369685649872\n",
      "Epoch:12, Step:100/196, Loss:0.4846249932050705\n",
      "Epoch:12, Step:150/196, Loss:0.4852244351307551\n",
      "Epoch:12, Step:196/196, Loss:0.48530193448674924\n",
      "Best AUC:0.757979282740569\n",
      "1669556143.2670658\n",
      "Epoch:13, Step:50/196, Loss:0.4823653358221054\n",
      "Epoch:13, Step:100/196, Loss:0.4840587535500526\n",
      "Epoch:13, Step:150/196, Loss:0.4846381483475367\n",
      "Epoch:13, Step:196/196, Loss:0.4851214706289525\n",
      "Best AUC:0.757979282740569\n",
      "1669556148.496104\n",
      "Epoch:14, Step:50/196, Loss:0.4815211880207062\n",
      "Epoch:14, Step:100/196, Loss:0.4835072898864746\n",
      "Epoch:14, Step:150/196, Loss:0.4855444480975469\n",
      "Epoch:14, Step:196/196, Loss:0.48548925972106505\n",
      "Best AUC:0.757979282740569\n",
      "1669556153.7376363\n",
      "Epoch:15, Step:50/196, Loss:0.48011396527290345\n",
      "Epoch:15, Step:100/196, Loss:0.48209435284137725\n",
      "Epoch:15, Step:150/196, Loss:0.48368775129318237\n",
      "Epoch:15, Step:196/196, Loss:0.4849346211674262\n",
      "Best AUC:0.757979282740569\n",
      "1669556158.8897564\n",
      "Epoch:16, Step:50/196, Loss:0.4837974941730499\n",
      "Epoch:16, Step:100/196, Loss:0.48474096149206164\n",
      "Epoch:16, Step:150/196, Loss:0.4850046104192734\n",
      "Epoch:16, Step:196/196, Loss:0.4854655166973873\n",
      "Best AUC:0.757979282740569\n",
      "1669556164.4256835\n",
      "Epoch:17, Step:50/196, Loss:0.48058766782283785\n",
      "Epoch:17, Step:100/196, Loss:0.4827048915624619\n",
      "Epoch:17, Step:150/196, Loss:0.48413822253545125\n",
      "Epoch:17, Step:196/196, Loss:0.4848639466324631\n",
      "Best AUC:0.7595226847351108\n",
      "1669556169.7499733\n",
      "Epoch:18, Step:50/196, Loss:0.4844213563203812\n",
      "Epoch:18, Step:100/196, Loss:0.4842163419723511\n",
      "Epoch:18, Step:150/196, Loss:0.4848393601179123\n",
      "Epoch:18, Step:196/196, Loss:0.4846981867235534\n",
      "Best AUC:0.7597597344266809\n",
      "1669556174.8445928\n",
      "Epoch:19, Step:50/196, Loss:0.48189449310302734\n",
      "Epoch:19, Step:100/196, Loss:0.48332635432481763\n",
      "Epoch:19, Step:150/196, Loss:0.48465941846370697\n",
      "Epoch:19, Step:196/196, Loss:0.4849505558305857\n",
      "Best AUC:0.7597597344266809\n",
      "1669556179.9582443\n",
      "Epoch:20, Step:50/196, Loss:0.48246907532215116\n",
      "Epoch:20, Step:100/196, Loss:0.48560612469911574\n",
      "Epoch:20, Step:150/196, Loss:0.48521105806032816\n",
      "Epoch:20, Step:196/196, Loss:0.4850868429152333\n",
      "Best AUC:0.7597597344266809\n",
      "1669556185.0886683\n",
      "Epoch:21, Step:50/196, Loss:0.4830207258462906\n",
      "Epoch:21, Step:100/196, Loss:0.48390054643154146\n",
      "Epoch:21, Step:150/196, Loss:0.4846024570862452\n",
      "Epoch:21, Step:196/196, Loss:0.4846053176692554\n",
      "Best AUC:0.7597597344266809\n",
      "1669556190.120893\n",
      "Epoch:22, Step:50/196, Loss:0.4817794191837311\n",
      "Epoch:22, Step:100/196, Loss:0.48401127487421036\n",
      "Epoch:22, Step:150/196, Loss:0.48455029567082725\n",
      "Epoch:22, Step:196/196, Loss:0.48471551540554786\n",
      "Best AUC:0.7597597344266809\n",
      "1669556195.288303\n",
      "Epoch:23, Step:50/196, Loss:0.4840280330181122\n",
      "Epoch:23, Step:100/196, Loss:0.48465224385261535\n",
      "Epoch:23, Step:150/196, Loss:0.48481722434361774\n",
      "Epoch:23, Step:196/196, Loss:0.48470442666082963\n",
      "Best AUC:0.7597597344266809\n",
      "1669556200.4647145\n",
      "Epoch:24, Step:50/196, Loss:0.48220186710357665\n",
      "Epoch:24, Step:100/196, Loss:0.4838369834423065\n",
      "Epoch:24, Step:150/196, Loss:0.4844078441460927\n",
      "Epoch:24, Step:196/196, Loss:0.48453824769477455\n",
      "Best AUC:0.7597597344266809\n",
      "1669556205.749623\n",
      "Epoch:25, Step:50/196, Loss:0.48035349667072297\n",
      "Epoch:25, Step:100/196, Loss:0.48232339531183244\n",
      "Epoch:25, Step:150/196, Loss:0.4839017866055171\n",
      "Epoch:25, Step:196/196, Loss:0.48419439214832927\n",
      "Best AUC:0.7600728196344462\n",
      "1669556211.1825206\n",
      "Epoch:26, Step:50/196, Loss:0.4793266868591309\n",
      "Epoch:26, Step:100/196, Loss:0.48301283836364745\n",
      "Epoch:26, Step:150/196, Loss:0.4836256678899129\n",
      "Epoch:26, Step:196/196, Loss:0.48409932471659717\n",
      "Best AUC:0.7600728196344462\n",
      "1669556216.4413145\n",
      "Epoch:27, Step:50/196, Loss:0.4814243978261948\n",
      "Epoch:27, Step:100/196, Loss:0.4848320436477661\n",
      "Epoch:27, Step:150/196, Loss:0.48489614367485045\n",
      "Epoch:27, Step:196/196, Loss:0.48450635951392507\n",
      "Best AUC:0.7600728196344462\n",
      "1669556221.5933092\n",
      "Epoch:28, Step:50/196, Loss:0.4824729514122009\n",
      "Epoch:28, Step:100/196, Loss:0.4827000489830971\n",
      "Epoch:28, Step:150/196, Loss:0.48328969756762186\n",
      "Epoch:28, Step:196/196, Loss:0.4833597124231105\n",
      "Best AUC:0.7600728196344462\n",
      "1669556226.6863644\n",
      "Epoch:29, Step:50/196, Loss:0.4803174549341202\n",
      "Epoch:29, Step:100/196, Loss:0.48235023230314256\n",
      "Epoch:29, Step:150/196, Loss:0.48275753438472746\n",
      "Epoch:29, Step:196/196, Loss:0.483446441590786\n",
      "Best AUC:0.7600728196344462\n",
      "1669556231.7917817\n",
      "Epoch:30, Step:50/196, Loss:0.47871207296848295\n",
      "Epoch:30, Step:100/196, Loss:0.4822346466779709\n",
      "Epoch:30, Step:150/196, Loss:0.4836313811937968\n",
      "Epoch:30, Step:196/196, Loss:0.48381170189502287\n",
      "Best AUC:0.7600728196344462\n",
      "1669556237.0881016\n",
      "Epoch:31, Step:50/196, Loss:0.48348688304424287\n",
      "Epoch:31, Step:100/196, Loss:0.4840361976623535\n",
      "Epoch:31, Step:150/196, Loss:0.48279988805452984\n",
      "Epoch:31, Step:196/196, Loss:0.4836896498288427\n",
      "Best AUC:0.7606766086586189\n",
      "1669556242.3317258\n",
      "Epoch:32, Step:50/196, Loss:0.4796295964717865\n",
      "Epoch:32, Step:100/196, Loss:0.48294316440820695\n",
      "Epoch:32, Step:150/196, Loss:0.4832450274626414\n",
      "Epoch:32, Step:196/196, Loss:0.48362052060511646\n",
      "Best AUC:0.7606766086586189\n",
      "1669556247.5631123\n",
      "Epoch:33, Step:50/196, Loss:0.48201872408390045\n",
      "Epoch:33, Step:100/196, Loss:0.4830850392580032\n",
      "Epoch:33, Step:150/196, Loss:0.48345772763093314\n",
      "Epoch:33, Step:196/196, Loss:0.4831780146579353\n",
      "Best AUC:0.7606766086586189\n",
      "1669556252.8458705\n",
      "Epoch:34, Step:50/196, Loss:0.4822524350881576\n",
      "Epoch:34, Step:100/196, Loss:0.48277465999126434\n",
      "Epoch:34, Step:150/196, Loss:0.4826652830839157\n",
      "Epoch:34, Step:196/196, Loss:0.4831572314914392\n",
      "Best AUC:0.7610882025429792\n",
      "1669556258.0933788\n",
      "Epoch:35, Step:50/196, Loss:0.4807922053337097\n",
      "Epoch:35, Step:100/196, Loss:0.4802751809358597\n",
      "Epoch:35, Step:150/196, Loss:0.482176095644633\n",
      "Epoch:35, Step:196/196, Loss:0.48270440010391935\n",
      "Best AUC:0.7619102596329538\n",
      "1669556263.4800434\n",
      "Epoch:36, Step:50/196, Loss:0.4802968055009842\n",
      "Epoch:36, Step:100/196, Loss:0.48170216649770736\n",
      "Epoch:36, Step:150/196, Loss:0.4822344358762105\n",
      "Epoch:36, Step:196/196, Loss:0.4822988317024951\n",
      "Best AUC:0.7619102596329538\n",
      "1669556268.891312\n",
      "Epoch:37, Step:50/196, Loss:0.47808068335056303\n",
      "Epoch:37, Step:100/196, Loss:0.47903227478265764\n",
      "Epoch:37, Step:150/196, Loss:0.4814794784784317\n",
      "Epoch:37, Step:196/196, Loss:0.4821668679312784\n",
      "Best AUC:0.7619102596329538\n",
      "1669556274.0567381\n",
      "Epoch:38, Step:50/196, Loss:0.4777092432975769\n",
      "Epoch:38, Step:100/196, Loss:0.4788783848285675\n",
      "Epoch:38, Step:150/196, Loss:0.4808683621883392\n",
      "Epoch:38, Step:196/196, Loss:0.48185741277981775\n",
      "Best AUC:0.7619102596329538\n",
      "1669556279.2633708\n",
      "Epoch:39, Step:50/196, Loss:0.47918331921100615\n",
      "Epoch:39, Step:100/196, Loss:0.4809400990605354\n",
      "Epoch:39, Step:150/196, Loss:0.482188659509023\n",
      "Epoch:39, Step:196/196, Loss:0.4820928388104147\n",
      "Best AUC:0.7619430969102797\n",
      "1669556284.4242153\n",
      "Epoch:40, Step:50/196, Loss:0.4779187524318695\n",
      "Epoch:40, Step:100/196, Loss:0.47953799307346345\n",
      "Epoch:40, Step:150/196, Loss:0.48088739554087323\n",
      "Epoch:40, Step:196/196, Loss:0.48147025385073255\n",
      "Best AUC:0.7619430969102797\n",
      "1669556289.5734787\n",
      "Epoch:41, Step:50/196, Loss:0.48041791796684263\n",
      "Epoch:41, Step:100/196, Loss:0.4814017540216446\n",
      "Epoch:41, Step:150/196, Loss:0.48189237813154856\n",
      "Epoch:41, Step:196/196, Loss:0.48158635199069977\n",
      "Best AUC:0.7623511778020705\n",
      "1669556294.7718577\n",
      "Epoch:42, Step:50/196, Loss:0.47957837462425235\n",
      "Epoch:42, Step:100/196, Loss:0.48046150922775266\n",
      "Epoch:42, Step:150/196, Loss:0.48152117371559144\n",
      "Epoch:42, Step:196/196, Loss:0.4813721759282813\n",
      "Best AUC:0.7623511778020705\n",
      "1669556299.8029563\n",
      "Epoch:43, Step:50/196, Loss:0.47958352863788606\n",
      "Epoch:43, Step:100/196, Loss:0.4808619755506516\n",
      "Epoch:43, Step:150/196, Loss:0.48126686533292135\n",
      "Epoch:43, Step:196/196, Loss:0.481494634279183\n",
      "Best AUC:0.7623511778020705\n",
      "1669556304.9395974\n",
      "Epoch:44, Step:50/196, Loss:0.47492662727832796\n",
      "Epoch:44, Step:100/196, Loss:0.4796134877204895\n",
      "Epoch:44, Step:150/196, Loss:0.4809517620007197\n",
      "Epoch:44, Step:196/196, Loss:0.48097933448699054\n",
      "Best AUC:0.7624450748870727\n",
      "1669556310.0862272\n",
      "Epoch:45, Step:50/196, Loss:0.4785020524263382\n",
      "Epoch:45, Step:100/196, Loss:0.48074591279029844\n",
      "Epoch:45, Step:150/196, Loss:0.48181344866752623\n",
      "Epoch:45, Step:196/196, Loss:0.4812088818574438\n",
      "Best AUC:0.7624450748870727\n",
      "1669556315.2644398\n",
      "Epoch:46, Step:50/196, Loss:0.4768020564317703\n",
      "Epoch:46, Step:100/196, Loss:0.478950762450695\n",
      "Epoch:46, Step:150/196, Loss:0.47953109701474506\n",
      "Epoch:46, Step:196/196, Loss:0.4805711897052064\n",
      "Best AUC:0.7631158413759407\n",
      "1669556320.4086556\n",
      "Epoch:47, Step:50/196, Loss:0.47628057122230527\n",
      "Epoch:47, Step:100/196, Loss:0.47910828560590746\n",
      "Epoch:47, Step:150/196, Loss:0.4804294679562251\n",
      "Epoch:47, Step:196/196, Loss:0.48047580022592934\n",
      "Best AUC:0.7631158413759407\n",
      "1669556325.471259\n",
      "Epoch:48, Step:50/196, Loss:0.47687901198863986\n",
      "Epoch:48, Step:100/196, Loss:0.4780743420124054\n",
      "Epoch:48, Step:150/196, Loss:0.4798797442515691\n",
      "Epoch:48, Step:196/196, Loss:0.48046464837935504\n",
      "Best AUC:0.7636926857996947\n",
      "1669556330.5177226\n",
      "Epoch:49, Step:50/196, Loss:0.47708654165267944\n",
      "Epoch:49, Step:100/196, Loss:0.4797708860039711\n",
      "Epoch:49, Step:150/196, Loss:0.47991218805313113\n",
      "Epoch:49, Step:196/196, Loss:0.4804219418034262\n",
      "Best AUC:0.7636926857996947\n",
      "1669556335.5814567\n",
      "Epoch:50, Step:50/196, Loss:0.4754144048690796\n",
      "Epoch:50, Step:100/196, Loss:0.4787793552875519\n",
      "Epoch:50, Step:150/196, Loss:0.47900541285673776\n",
      "Epoch:50, Step:196/196, Loss:0.48026606425338864\n",
      "Best AUC:0.7636926857996947\n"
     ]
    }
   ],
   "source": [
    "# train and eval\n",
    "model.fit(train_loader, test_loader, epochs=50)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "FactorizationMachine(\n  (fm_1st_int_vec): Linear(in_features=13, out_features=1, bias=True)\n  (fm_1st_cate_emb): ModuleList(\n    (0): Embedding(1036, 1)\n    (1): Embedding(526, 1)\n    (2): Embedding(177397, 1)\n    (3): Embedding(75762, 1)\n    (4): Embedding(234, 1)\n    (5): Embedding(14, 1)\n    (6): Embedding(10165, 1)\n    (7): Embedding(482, 1)\n    (8): Embedding(3, 1)\n    (9): Embedding(23499, 1)\n    (10): Embedding(4501, 1)\n    (11): Embedding(152106, 1)\n    (12): Embedding(3029, 1)\n    (13): Embedding(26, 1)\n    (14): Embedding(7825, 1)\n    (15): Embedding(120291, 1)\n    (16): Embedding(10, 1)\n    (17): Embedding(3512, 1)\n    (18): Embedding(1704, 1)\n    (19): Embedding(4, 1)\n    (20): Embedding(138137, 1)\n    (21): Embedding(14, 1)\n    (22): Embedding(15, 1)\n    (23): Embedding(29183, 1)\n    (24): Embedding(63, 1)\n    (25): Embedding(22354, 1)\n  )\n  (fm_2nd_cate_emb): ModuleList(\n    (0): Embedding(1036, 8)\n    (1): Embedding(526, 8)\n    (2): Embedding(177397, 8)\n    (3): Embedding(75762, 8)\n    (4): Embedding(234, 8)\n    (5): Embedding(14, 8)\n    (6): Embedding(10165, 8)\n    (7): Embedding(482, 8)\n    (8): Embedding(3, 8)\n    (9): Embedding(23499, 8)\n    (10): Embedding(4501, 8)\n    (11): Embedding(152106, 8)\n    (12): Embedding(3029, 8)\n    (13): Embedding(26, 8)\n    (14): Embedding(7825, 8)\n    (15): Embedding(120291, 8)\n    (16): Embedding(10, 8)\n    (17): Embedding(3512, 8)\n    (18): Embedding(1704, 8)\n    (19): Embedding(4, 8)\n    (20): Embedding(138137, 8)\n    (21): Embedding(14, 8)\n    (22): Embedding(15, 8)\n    (23): Embedding(29183, 8)\n    (24): Embedding(63, 8)\n    (25): Embedding(22354, 8)\n  )\n  (sigmoid): Sigmoid()\n)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from FM import FactorizationMachine\n",
    "base_model = FactorizationMachine(39, integer_features, categorical_features, cate_each_features_size,device='cuda')\n",
    "base_model.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1669557175.3757775\n",
      "Epoch:1, Step:50/196, Loss:20.744446754455566\n",
      "Epoch:1, Step:100/196, Loss:17.9903546333313\n",
      "Epoch:1, Step:150/196, Loss:16.136262035369874\n",
      "Epoch:1, Step:196/196, Loss:14.585182112090441\n",
      "Best AUC:0.6270172752419328\n",
      "1669557182.1511912\n",
      "Epoch:2, Step:50/196, Loss:5.952697582244873\n",
      "Epoch:2, Step:100/196, Loss:4.551464211940766\n",
      "Epoch:2, Step:150/196, Loss:3.605742723941803\n",
      "Epoch:2, Step:196/196, Loss:3.0231840954143174\n",
      "Best AUC:0.6353366201333568\n",
      "1669557187.1526775\n",
      "Epoch:3, Step:50/196, Loss:0.8065355741977691\n",
      "Epoch:3, Step:100/196, Loss:0.7456582975387573\n",
      "Epoch:3, Step:150/196, Loss:0.7003594823678334\n",
      "Epoch:3, Step:196/196, Loss:0.6703778359354758\n",
      "Best AUC:0.6796058202932731\n",
      "1669557192.2644851\n",
      "Epoch:4, Step:50/196, Loss:0.5311242336034775\n",
      "Epoch:4, Step:100/196, Loss:0.526192232966423\n",
      "Epoch:4, Step:150/196, Loss:0.5233119157950084\n",
      "Epoch:4, Step:196/196, Loss:0.5204556411018177\n",
      "Best AUC:0.7201034137818729\n",
      "1669557197.209048\n",
      "Epoch:5, Step:50/196, Loss:0.49944277465343473\n",
      "Epoch:5, Step:100/196, Loss:0.500839156806469\n",
      "Epoch:5, Step:150/196, Loss:0.5014892941713334\n",
      "Epoch:5, Step:196/196, Loss:0.5013513104343901\n",
      "Best AUC:0.7329415325852064\n",
      "1669557202.1609159\n",
      "Epoch:6, Step:50/196, Loss:0.4942218202352524\n",
      "Epoch:6, Step:100/196, Loss:0.4953724232316017\n",
      "Epoch:6, Step:150/196, Loss:0.49668276449044546\n",
      "Epoch:6, Step:196/196, Loss:0.4970534778371149\n",
      "Best AUC:0.7384073669591429\n",
      "1669557207.1871433\n",
      "Epoch:7, Step:50/196, Loss:0.49222403287887573\n",
      "Epoch:7, Step:100/196, Loss:0.49418182045221326\n",
      "Epoch:7, Step:150/196, Loss:0.49503851691881817\n",
      "Epoch:7, Step:196/196, Loss:0.4954997159692706\n",
      "Best AUC:0.7397783096799126\n",
      "1669557212.1005287\n",
      "Epoch:8, Step:50/196, Loss:0.4927500194311142\n",
      "Epoch:8, Step:100/196, Loss:0.4940010517835617\n",
      "Epoch:8, Step:150/196, Loss:0.49556538542111717\n",
      "Epoch:8, Step:196/196, Loss:0.49518105159608683\n",
      "Best AUC:0.7417690150231044\n",
      "1669557217.0241938\n",
      "Epoch:9, Step:50/196, Loss:0.4926284897327423\n",
      "Epoch:9, Step:100/196, Loss:0.49400934755802156\n",
      "Epoch:9, Step:150/196, Loss:0.49494170387585956\n",
      "Epoch:9, Step:196/196, Loss:0.49465026022220143\n",
      "Best AUC:0.7417690150231044\n",
      "1669557222.1534133\n",
      "Epoch:10, Step:50/196, Loss:0.4896478062868118\n",
      "Epoch:10, Step:100/196, Loss:0.49344143211841585\n",
      "Epoch:10, Step:150/196, Loss:0.494454400340716\n",
      "Epoch:10, Step:196/196, Loss:0.4949310814239541\n",
      "Best AUC:0.7417690150231044\n",
      "1669557227.0539842\n",
      "Epoch:11, Step:50/196, Loss:0.4873225063085556\n",
      "Epoch:11, Step:100/196, Loss:0.49181256711483\n",
      "Epoch:11, Step:150/196, Loss:0.49437996864318845\n",
      "Epoch:11, Step:196/196, Loss:0.49480724806080056\n",
      "Best AUC:0.7417690150231044\n",
      "1669557232.0907767\n",
      "Epoch:12, Step:50/196, Loss:0.4921435135602951\n",
      "Epoch:12, Step:100/196, Loss:0.4930119976401329\n",
      "Epoch:12, Step:150/196, Loss:0.49469935456911723\n",
      "Epoch:12, Step:196/196, Loss:0.49536192675634305\n",
      "Best AUC:0.7417690150231044\n",
      "1669557237.0508099\n",
      "Epoch:13, Step:50/196, Loss:0.493048290014267\n",
      "Epoch:13, Step:100/196, Loss:0.49497332334518435\n",
      "Epoch:13, Step:150/196, Loss:0.49544095774491625\n",
      "Epoch:13, Step:196/196, Loss:0.49500841723412886\n",
      "Best AUC:0.7417690150231044\n",
      "1669557242.107425\n",
      "Epoch:14, Step:50/196, Loss:0.49362550020217894\n",
      "Epoch:14, Step:100/196, Loss:0.49436140328645706\n",
      "Epoch:14, Step:150/196, Loss:0.4950605483849843\n",
      "Epoch:14, Step:196/196, Loss:0.4955813820872988\n",
      "Best AUC:0.7417690150231044\n",
      "1669557247.0380661\n",
      "Epoch:15, Step:50/196, Loss:0.49073644638061525\n",
      "Epoch:15, Step:100/196, Loss:0.4946098077297211\n",
      "Epoch:15, Step:150/196, Loss:0.494765700896581\n",
      "Epoch:15, Step:196/196, Loss:0.49587680840370607\n",
      "Best AUC:0.7417690150231044\n",
      "1669557252.0414405\n",
      "Epoch:16, Step:50/196, Loss:0.49269129931926725\n",
      "Epoch:16, Step:100/196, Loss:0.49449779719114306\n",
      "Epoch:16, Step:150/196, Loss:0.49559101919333143\n",
      "Epoch:16, Step:196/196, Loss:0.49568692685998217\n",
      "Best AUC:0.7417690150231044\n",
      "1669557257.0072832\n",
      "Epoch:17, Step:50/196, Loss:0.493671715259552\n",
      "Epoch:17, Step:100/196, Loss:0.4962680596113205\n",
      "Epoch:17, Step:150/196, Loss:0.49605619311332705\n",
      "Epoch:17, Step:196/196, Loss:0.49622117880047584\n",
      "Best AUC:0.7417690150231044\n",
      "1669557261.9758124\n",
      "Epoch:18, Step:50/196, Loss:0.4931819236278534\n",
      "Epoch:18, Step:100/196, Loss:0.4937880745530128\n",
      "Epoch:18, Step:150/196, Loss:0.4963336503505707\n",
      "Epoch:18, Step:196/196, Loss:0.49634590729766964\n",
      "Best AUC:0.7417690150231044\n",
      "1669557266.908591\n",
      "Epoch:19, Step:50/196, Loss:0.4946144878864288\n",
      "Epoch:19, Step:100/196, Loss:0.49511870592832563\n",
      "Epoch:19, Step:150/196, Loss:0.4960227580865224\n",
      "Epoch:19, Step:196/196, Loss:0.49625276941425944\n",
      "Best AUC:0.7417690150231044\n",
      "1669557271.8509562\n",
      "Epoch:20, Step:50/196, Loss:0.4952121102809906\n",
      "Epoch:20, Step:100/196, Loss:0.4966613507270813\n",
      "Epoch:20, Step:150/196, Loss:0.49739635348320005\n",
      "Epoch:20, Step:196/196, Loss:0.49673744990509383\n",
      "Best AUC:0.7417690150231044\n",
      "1669557276.993904\n",
      "Epoch:21, Step:50/196, Loss:0.4935370886325836\n",
      "Epoch:21, Step:100/196, Loss:0.4950438955426216\n",
      "Epoch:21, Step:150/196, Loss:0.49581016063690186\n",
      "Epoch:21, Step:196/196, Loss:0.4963070047753198\n",
      "Best AUC:0.7417690150231044\n",
      "1669557281.9366992\n",
      "Epoch:22, Step:50/196, Loss:0.49093774020671843\n",
      "Epoch:22, Step:100/196, Loss:0.4954156795144081\n",
      "Epoch:22, Step:150/196, Loss:0.49611017127831775\n",
      "Epoch:22, Step:196/196, Loss:0.49651026558511113\n",
      "Best AUC:0.7417690150231044\n",
      "1669557286.9710214\n",
      "Epoch:23, Step:50/196, Loss:0.49576297402381897\n",
      "Epoch:23, Step:100/196, Loss:0.49621218293905256\n",
      "Epoch:23, Step:150/196, Loss:0.496428878903389\n",
      "Epoch:23, Step:196/196, Loss:0.4966983595977024\n",
      "Best AUC:0.7417690150231044\n",
      "1669557291.8886905\n",
      "Epoch:24, Step:50/196, Loss:0.4940371084213257\n",
      "Epoch:24, Step:100/196, Loss:0.4969239801168442\n",
      "Epoch:24, Step:150/196, Loss:0.4967012693484624\n",
      "Epoch:24, Step:196/196, Loss:0.4965873562863895\n",
      "Best AUC:0.7417690150231044\n",
      "1669557296.8460124\n",
      "Epoch:25, Step:50/196, Loss:0.4929663223028183\n",
      "Epoch:25, Step:100/196, Loss:0.4945198193192482\n",
      "Epoch:25, Step:150/196, Loss:0.49580671966075895\n",
      "Epoch:25, Step:196/196, Loss:0.49655456673734044\n",
      "Best AUC:0.7417690150231044\n",
      "1669557301.9500332\n",
      "Epoch:26, Step:50/196, Loss:0.49292524814605715\n",
      "Epoch:26, Step:100/196, Loss:0.4954874932765961\n",
      "Epoch:26, Step:150/196, Loss:0.4953322658936183\n",
      "Epoch:26, Step:196/196, Loss:0.4963640172262581\n",
      "Best AUC:0.7417690150231044\n",
      "1669557306.9322941\n",
      "Epoch:27, Step:50/196, Loss:0.4947426784038544\n",
      "Epoch:27, Step:100/196, Loss:0.49573767244815825\n",
      "Epoch:27, Step:150/196, Loss:0.4961882213751475\n",
      "Epoch:27, Step:196/196, Loss:0.4963407484548433\n",
      "Best AUC:0.7417690150231044\n",
      "1669557311.8798985\n",
      "Epoch:28, Step:50/196, Loss:0.49438838064670565\n",
      "Epoch:28, Step:100/196, Loss:0.49562656790018084\n",
      "Epoch:28, Step:150/196, Loss:0.49673716803391776\n",
      "Epoch:28, Step:196/196, Loss:0.49652809528063757\n",
      "Best AUC:0.7417690150231044\n",
      "1669557316.8513372\n",
      "Epoch:29, Step:50/196, Loss:0.494232172369957\n",
      "Epoch:29, Step:100/196, Loss:0.49629157841205596\n",
      "Epoch:29, Step:150/196, Loss:0.4954705454905828\n",
      "Epoch:29, Step:196/196, Loss:0.4960541605037086\n",
      "Best AUC:0.7417690150231044\n",
      "1669557321.7316658\n",
      "Epoch:30, Step:50/196, Loss:0.4930772596597672\n",
      "Epoch:30, Step:100/196, Loss:0.4959739601612091\n",
      "Epoch:30, Step:150/196, Loss:0.4964690363407135\n",
      "Epoch:30, Step:196/196, Loss:0.4961634319351644\n",
      "Best AUC:0.7417690150231044\n",
      "1669557326.618298\n",
      "Epoch:31, Step:50/196, Loss:0.49348329603672025\n",
      "Epoch:31, Step:100/196, Loss:0.4940996488928795\n",
      "Epoch:31, Step:150/196, Loss:0.49520869354406993\n",
      "Epoch:31, Step:196/196, Loss:0.4961721570212014\n",
      "Best AUC:0.7417690150231044\n",
      "1669557331.6026816\n",
      "Epoch:32, Step:50/196, Loss:0.4947733664512634\n",
      "Epoch:32, Step:100/196, Loss:0.4962513157725334\n",
      "Epoch:32, Step:150/196, Loss:0.49707019011179604\n",
      "Epoch:32, Step:196/196, Loss:0.49610928385233394\n",
      "Best AUC:0.7417690150231044\n",
      "1669557336.5238256\n",
      "Epoch:33, Step:50/196, Loss:0.4947882944345474\n",
      "Epoch:33, Step:100/196, Loss:0.495100237429142\n",
      "Epoch:33, Step:150/196, Loss:0.4960001754760742\n",
      "Epoch:33, Step:196/196, Loss:0.49595086352557555\n",
      "Best AUC:0.7417690150231044\n",
      "1669557341.6711142\n",
      "Epoch:34, Step:50/196, Loss:0.4935809934139252\n",
      "Epoch:34, Step:100/196, Loss:0.495866074860096\n",
      "Epoch:34, Step:150/196, Loss:0.4955801375706991\n",
      "Epoch:34, Step:196/196, Loss:0.49566542661311674\n",
      "Best AUC:0.7417690150231044\n",
      "1669557346.886261\n",
      "Epoch:35, Step:50/196, Loss:0.49238785207271574\n",
      "Epoch:35, Step:100/196, Loss:0.4941549402475357\n",
      "Epoch:35, Step:150/196, Loss:0.49516361951828003\n",
      "Epoch:35, Step:196/196, Loss:0.49566666523412783\n",
      "Best AUC:0.7417690150231044\n",
      "1669557352.002539\n",
      "Epoch:36, Step:50/196, Loss:0.4932262486219406\n",
      "Epoch:36, Step:100/196, Loss:0.49531748235225675\n",
      "Epoch:36, Step:150/196, Loss:0.495797731479009\n",
      "Epoch:36, Step:196/196, Loss:0.49570177100142654\n",
      "Best AUC:0.7417690150231044\n",
      "1669557357.194709\n",
      "Epoch:37, Step:50/196, Loss:0.4934890031814575\n",
      "Epoch:37, Step:100/196, Loss:0.4935475215315819\n",
      "Epoch:37, Step:150/196, Loss:0.4949081965287526\n",
      "Epoch:37, Step:196/196, Loss:0.4956755054240324\n",
      "Best AUC:0.7417965002457327\n",
      "1669557362.350037\n",
      "Epoch:38, Step:50/196, Loss:0.49424092829227445\n",
      "Epoch:38, Step:100/196, Loss:0.4953101646900177\n",
      "Epoch:38, Step:150/196, Loss:0.49515972514947254\n",
      "Epoch:38, Step:196/196, Loss:0.49562759119637156\n",
      "Best AUC:0.7417965002457327\n",
      "1669557367.458053\n",
      "Epoch:39, Step:50/196, Loss:0.4941879194974899\n",
      "Epoch:39, Step:100/196, Loss:0.4951951822638512\n",
      "Epoch:39, Step:150/196, Loss:0.4952474009990692\n",
      "Epoch:39, Step:196/196, Loss:0.4956599236751089\n",
      "Best AUC:0.7417965002457327\n",
      "1669557372.459679\n",
      "Epoch:40, Step:50/196, Loss:0.49306246876716614\n",
      "Epoch:40, Step:100/196, Loss:0.495245481133461\n",
      "Epoch:40, Step:150/196, Loss:0.49558282613754273\n",
      "Epoch:40, Step:196/196, Loss:0.4955937027627108\n",
      "Best AUC:0.7417965002457327\n",
      "1669557377.6979823\n",
      "Epoch:41, Step:50/196, Loss:0.4915346366167068\n",
      "Epoch:41, Step:100/196, Loss:0.49546802788972855\n",
      "Epoch:41, Step:150/196, Loss:0.4954162510236104\n",
      "Epoch:41, Step:196/196, Loss:0.4956812250370882\n",
      "Best AUC:0.7417965002457327\n",
      "1669557382.9080615\n",
      "Epoch:42, Step:50/196, Loss:0.4921105718612671\n",
      "Epoch:42, Step:100/196, Loss:0.4944914871454239\n",
      "Epoch:42, Step:150/196, Loss:0.49577566107114157\n",
      "Epoch:42, Step:196/196, Loss:0.49571866983053636\n",
      "Best AUC:0.7417965002457327\n",
      "1669557387.8504305\n",
      "Epoch:43, Step:50/196, Loss:0.4935776060819626\n",
      "Epoch:43, Step:100/196, Loss:0.49574457615613937\n",
      "Epoch:43, Step:150/196, Loss:0.4964000197251638\n",
      "Epoch:43, Step:196/196, Loss:0.49586821666785647\n",
      "Best AUC:0.7417965002457327\n",
      "1669557392.8574557\n",
      "Epoch:44, Step:50/196, Loss:0.49399368464946747\n",
      "Epoch:44, Step:100/196, Loss:0.4944771537184715\n",
      "Epoch:44, Step:150/196, Loss:0.4956892079114914\n",
      "Epoch:44, Step:196/196, Loss:0.49540446546612954\n",
      "Best AUC:0.7417965002457327\n",
      "1669557397.939515\n",
      "Epoch:45, Step:50/196, Loss:0.4920873034000397\n",
      "Epoch:45, Step:100/196, Loss:0.49415846467018126\n",
      "Epoch:45, Step:150/196, Loss:0.49507680157820383\n",
      "Epoch:45, Step:196/196, Loss:0.49542687924540774\n",
      "Best AUC:0.7417965002457327\n",
      "1669557402.9838567\n",
      "Epoch:46, Step:50/196, Loss:0.49204348146915433\n",
      "Epoch:46, Step:100/196, Loss:0.493411665558815\n",
      "Epoch:46, Step:150/196, Loss:0.49446747501691185\n",
      "Epoch:46, Step:196/196, Loss:0.4953651438866343\n",
      "Best AUC:0.7417965002457327\n",
      "1669557408.2376132\n",
      "Epoch:47, Step:50/196, Loss:0.49063493967056276\n",
      "Epoch:47, Step:100/196, Loss:0.49346435219049456\n",
      "Epoch:47, Step:150/196, Loss:0.49420241475105287\n",
      "Epoch:47, Step:196/196, Loss:0.49536637992275007\n",
      "Best AUC:0.7417965002457327\n",
      "1669557413.3648853\n",
      "Epoch:48, Step:50/196, Loss:0.4926333427429199\n",
      "Epoch:48, Step:100/196, Loss:0.49450906276702883\n",
      "Epoch:48, Step:150/196, Loss:0.49444995005925496\n",
      "Epoch:48, Step:196/196, Loss:0.49556078533737025\n",
      "Best AUC:0.7417965002457327\n",
      "1669557418.395846\n",
      "Epoch:49, Step:50/196, Loss:0.4894480860233307\n",
      "Epoch:49, Step:100/196, Loss:0.4928851920366287\n",
      "Epoch:49, Step:150/196, Loss:0.4951620835065842\n",
      "Epoch:49, Step:196/196, Loss:0.4957069596465753\n",
      "Best AUC:0.7417965002457327\n",
      "1669557423.4640722\n",
      "Epoch:50, Step:50/196, Loss:0.4926244032382965\n",
      "Epoch:50, Step:100/196, Loss:0.49387896686792376\n",
      "Epoch:50, Step:150/196, Loss:0.49532587269941963\n",
      "Epoch:50, Step:196/196, Loss:0.4951406481618784\n",
      "Best AUC:0.7417965002457327\n"
     ]
    }
   ],
   "source": [
    "base_model.fit(train_loader, test_loader, epochs=50)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
